{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d939fb5-b744-4080-ba71-246094187329",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Este código se ejecuta en una celda de un Notebook de Databricks\n",
    "# Su responsabilidad es detectar un único archivo de novedades,\n",
    "# validarlo, fusionarlo (MERGE) con la tabla 'category' y archivarlo.\n",
    "\n",
    "from pyspark.sql.functions import col, count, lit\n",
    "from pathlib import Path\n",
    "\n",
    "# 1. --- Detección Automática del Archivo a Procesar ---\n",
    "updates_volume_path = \"/Volumes/workspace/tecnomundo_data_raw/uploads_dimensions\"\n",
    "archive_path = f\"{updates_volume_path}/archive\" # Directorio para el historial\n",
    "\n",
    "print(f\"Buscando archivos de novedades en: {updates_volume_path}\")\n",
    "\n",
    "try:\n",
    "    # Listar todos los archivos y carpetas en el directorio del volumen\n",
    "    all_items = dbutils.fs.ls(updates_volume_path)\n",
    "    \n",
    "    # Filtrar para encontrar solo archivos CSV que no estén en la carpeta de archivo\n",
    "    csv_files_to_process = [f for f in all_items if f.name.lower().endswith('.csv') and not f.isDir()]\n",
    "    \n",
    "    # --- Validación de Archivo Único ---\n",
    "    if len(csv_files_to_process) == 0:\n",
    "        print(\"No se encontraron nuevos archivos CSV de novedades para procesar. Finalizando el proceso.\")\n",
    "        dbutils.notebook.exit(\"No hay archivos para procesar.\")\n",
    "    elif len(csv_files_to_process) > 1:\n",
    "        print(f\"Error: Se encontraron {len(csv_files_to_process)} archivos de novedades. Solo debe haber uno.\")\n",
    "        for f in csv_files_to_process:\n",
    "            print(f\" - {f.name}\")\n",
    "        dbutils.notebook.exit(\"Proceso abortado. Por favor, deje solo un archivo de novedades en el directorio.\")\n",
    "    \n",
    "    # Si pasamos las validaciones, tenemos exactamente un archivo para procesar\n",
    "    file_to_process_info = csv_files_to_process[0]\n",
    "    updates_csv_path = file_to_process_info.path\n",
    "    print(f\"Archivo único detectado para procesar: {updates_csv_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error al listar los archivos en el volumen: {e}\")\n",
    "    dbutils.notebook.exit(\"Fallo al acceder al directorio de novedades. Verifique los permisos del clúster.\")\n",
    "\n",
    "\n",
    "# 2. --- Configuración de Tablas ---\n",
    "target_dimension_table = \"workspace.tecnomundo_data_dimensions.category\"\n",
    "print(f\"Tabla de destino a actualizar: {target_dimension_table}\")\n",
    "\n",
    "\n",
    "# 3. --- Lectura y Preparación de las Novedades ---\n",
    "print(\"\\nLeyendo archivo CSV de novedades...\")\n",
    "try:\n",
    "    df_updates = (spark.read\n",
    "                  .format(\"csv\")\n",
    "                  .option(\"header\", \"true\")\n",
    "                  .load(updates_csv_path))\n",
    "    \n",
    "    # Creamos una vista temporal para usarla en las validaciones y el MERGE\n",
    "    df_updates.createOrReplaceTempView(\"category_updates_temp_view\")\n",
    "    print(\"Novedades leídas y listas para la fusión.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error al leer el archivo CSV desde el Volume: {e}\")\n",
    "    dbutils.notebook.exit(\"No se pudo leer el archivo de novedades. Verifica que la ruta y el nombre del archivo sean correctos.\")\n",
    "\n",
    "\n",
    "# 4. --- Validaciones Previas al MERGE ---\n",
    "print(\"\\n--- INICIANDO VALIDACIONES PREVIAS AL MERGE ---\")\n",
    "\n",
    "# --- Validación 1: Verificar la estructura del archivo de novedades ---\n",
    "print(\"Validación 1: Comprobando que el archivo de novedades tenga las columnas requeridas...\")\n",
    "required_columns = {\"codigo_producto\", \"nombre_del_producto\", \"categoria\"}\n",
    "actual_columns = set(df_updates.columns)\n",
    "\n",
    "if not required_columns.issubset(actual_columns):\n",
    "    missing_cols = required_columns - actual_columns\n",
    "    found_cols = list(actual_columns)\n",
    "    error_message = (f\"Proceso abortado: Estructura de archivo incorrecta. \"\n",
    "                     f\"Columnas requeridas que faltan: {missing_cols}. \"\n",
    "                     f\"Columnas encontradas en el archivo: {found_cols}.\")\n",
    "    print(f\"Error: {error_message}\")\n",
    "    dbutils.notebook.exit(error_message)\n",
    "print(\"  - OK: La estructura del archivo de novedades es correcta.\")\n",
    "\n",
    "\n",
    "# --- Validación 2: Comprobar si hay duplicados en el archivo de entrada ---\n",
    "print(\"\\nValidación 2: Comprobando duplicados en el archivo de novedades...\")\n",
    "duplicates_in_source_df = df_updates.groupBy(\"codigo_producto\").count().filter(col(\"count\") > 1)\n",
    "if duplicates_in_source_df.count() > 0:\n",
    "    print(\"Error: Se encontraron los siguientes códigos de producto duplicados en el archivo de entrada:\")\n",
    "    duplicates_in_source_df.show()\n",
    "    dbutils.notebook.exit(\"Proceso abortado debido a duplicados en el archivo de origen.\")\n",
    "print(\"  - OK: No se encontraron duplicados en el archivo de origen.\")\n",
    "\n",
    "\n",
    "# --- Validación 3: Análisis de Cambios ---\n",
    "print(\"\\nValidación 3: Analizando cambios a realizar (INSERT vs UPDATE)...\")\n",
    "df_to_update = spark.sql(f\"\"\"\n",
    "    SELECT source.codigo_producto FROM category_updates_temp_view source\n",
    "    INNER JOIN {target_dimension_table} target ON source.codigo_producto = target.codigo_producto\n",
    "\"\"\")\n",
    "print(f\"  - Se van a ACTUALIZAR {df_to_update.count()} productos existentes.\")\n",
    "if df_to_update.count() > 0: df_to_update.show(5, truncate=False)\n",
    "\n",
    "df_to_insert = spark.sql(f\"\"\"\n",
    "    SELECT source.codigo_producto FROM category_updates_temp_view source\n",
    "    LEFT JOIN {target_dimension_table} target ON source.codigo_producto = target.codigo_producto\n",
    "    WHERE target.codigo_producto IS NULL\n",
    "\"\"\")\n",
    "print(f\"  - Se van a INSERTAR {df_to_insert.count()} productos nuevos.\")\n",
    "if df_to_insert.count() > 0: df_to_insert.show(5, truncate=False)\n",
    "\n",
    "print(\"--- VALIDACIONES COMPLETADAS ---\")\n",
    "\n",
    "\n",
    "# 5. --- Fusión de Datos con MERGE INTO ---\n",
    "print(f\"\\nEjecutando MERGE en la tabla '{target_dimension_table}'...\")\n",
    "try:\n",
    "    merge_result = spark.sql(f\"\"\"\n",
    "      MERGE INTO {target_dimension_table} AS target\n",
    "      USING category_updates_temp_view AS source\n",
    "      ON target.codigo_producto = source.codigo_producto\n",
    "      WHEN MATCHED THEN\n",
    "        UPDATE SET\n",
    "          target.nombre_del_producto = source.nombre_del_producto,\n",
    "          target.categoria = source.categoria\n",
    "      WHEN NOT MATCHED THEN\n",
    "        INSERT (codigo_producto, nombre_del_producto, categoria)\n",
    "        VALUES (source.codigo_producto, source.nombre_del_producto, source.categoria)\n",
    "    \"\"\")\n",
    "    print(\"¡Éxito! La tabla de dimensiones ha sido actualizada.\")\n",
    "    merge_result.show()\n",
    "\n",
    "    # --- 6. Archivar el archivo procesado ---\n",
    "    print(\"\\nArchivando el archivo de novedades procesado...\")\n",
    "    dbutils.fs.mkdirs(archive_path) # Asegura que la carpeta de archivo exista\n",
    "    processed_file_name = Path(updates_csv_path).name\n",
    "    dbutils.fs.mv(updates_csv_path, f\"{archive_path}/{processed_file_name}\")\n",
    "    print(f\"Archivo procesado movido a: {archive_path}/{processed_file_name}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Ocurrió un error durante la operación MERGE o el archivado: {e}\")\n",
    "\n",
    "\n",
    "# 7. --- Verificación Final ---\n",
    "print(\"\\nMostrando una muestra de la tabla de dimensiones actualizada:\")\n",
    "display(spark.table(target_dimension_table).orderBy(col(\"codigo_producto\").desc()))\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "update_dim_category",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
